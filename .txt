Сейчас я скину весь код проекта

app/config/settings.py

from pathlib import Path

class Settings:
    RESULTS_DIR = Path("C:\\Users\\ta1\\Documents\\projects\\scaner\\app\\results")
    SUBDOMAINS_FILE = Path("C:\\Users\\ta1\\Documents\\projects\\scaner\\app\\subdomains.txt")
    MAX_CONTENT_LINES = 500
    REQUEST_TIMEOUT = 15
    DEEPSEEK_API_KEY = "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"
    DEEPSEEK_MODEL = "deepseek/deepseek-r1"
    MAX_RETRIES = 3
    RETRY_DELAY = 1.0

settings = Settings()

app/controllers/parser.py

from fastapi import APIRouter, HTTPException
from pathlib import Path
from urllib.parse import unquote
from app.services.crawler import CrawlerService
from app.services.ai_processor import AIProcessor
from app.utils.helpers import setup_logger, is_valid_url
import traceback

logger = setup_logger(__name__)

router = APIRouter()

# Инициализация сервисов
crawler = CrawlerService()
ai = AIProcessor()

@router.get("/parse/{url:path}")
async def parse_website(url: str):
    try:
        # Декодирование URL
        decoded_url = unquote(url)
        
        logger.info(f"Processing URL: {decoded_url}")
        
        # Проверка валидности URL
        if not is_valid_url(decoded_url):
            raise HTTPException(status_code=400, detail="Invalid URL format")
        
        # Обработка запроса
        result = await crawler.process_site(decoded_url)
        
        # Проверка наличия контента
        content_file = Path(result["content_file"])
        if not content_file.exists() or content_file.stat().st_size == 0:
            raise HTTPException(status_code=404, detail="No content found")
            
        # Генерация описания
        description = await ai.generate_description(content_file)
        
        return {
            "domain": result["domain"],
            "subdomains_count": len(result["subdomains"]),
            "pages_found": result["pages_found"],
            "description": description,
            "content_file": str(content_file)
        }
    except HTTPException as he:
        raise
    except Exception as e:
        logger.error(f"Critical error processing {url}: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail="Internal server error")

app/services/ai_processor.py

import httpx
from pathlib import Path
from app.config.settings import settings
import asyncio
from app.utils.helpers import (
    setup_logger,
    async_retry,
    timing_decorator,
)

logger = setup_logger(__name__)

class AIProcessor:
    def __init__(self):
        self.api_key = settings.DEEPSEEK_API_KEY
        self.model = settings.DEEPSEEK_MODEL
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"

    @async_retry(max_retries=settings.MAX_RETRIES, delay=settings.RETRY_DELAY)
    @timing_decorator
    async def generate_description(self, content_file: Path) -> str:
        """Генерация описания сайта (асинхронная версия)"""
        content = await self._read_limited_content_async(content_file)
        prompt = f"Опиши чем занимается компания: {content}"
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.base_url,
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={"model": self.model, "messages": [{"role": "user", "content": prompt}]},
                timeout=30
            )
        
        return response.json()["choices"][0]["message"]["content"]

    async def _read_limited_content_async(self, file_path: Path, word_limit=300) -> str:
        """Асинхронное чтение файла с ограничением по количеству слов"""
        loop = asyncio.get_event_loop()
        
        content = await loop.run_in_executor(None, lambda: file_path.read_text(encoding="utf-8"))
        
        words = content.split()[word_limit:word_limit*2]
        return " ".join(words)

app/services/crawler.py

import asyncio
import httpx
from pathlib import Path
from typing import List, Set
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from app.config.settings import settings
from app.utils.helpers import (
    setup_logger,
    async_retry,
    timing_decorator,
    clean_html_content,
)

logger = setup_logger(__name__)

class CrawlerService:
    def __init__(self):
        self.results_dir = Path(settings.RESULTS_DIR)
        self.results_dir.mkdir(exist_ok=True)

    def extract_domain(self, url: str) -> str:
        """Извлечение домена из URL"""
        from tldextract import extract
        extracted = extract(url)
        return f"{extracted.domain}.{extracted.suffix}"

    @async_retry(max_retries=settings.MAX_RETRIES, delay=settings.RETRY_DELAY)
    async def check_subdomain(self, session: httpx.AsyncClient, domain: str, subdomain: str) -> str:
        """Проверка доступности поддомена"""
        url = f"https://{subdomain}.{domain}"
        try:
            response = await session.head(url, follow_redirects=True, timeout=settings.REQUEST_TIMEOUT)
            if response.status_code < 400:
                return str(response.url)
        except Exception as e:
            logger.warning(f"Failed to check subdomain {url}: {e}")
        return ""

    @timing_decorator
    async def scan_subdomains(self, domain: str) -> List[str]:
        """Сканирование поддоменов"""
        subdomains = Path(settings.SUBDOMAINS_FILE).read_text().splitlines()
        async with httpx.AsyncClient(verify=False) as client:
            tasks = [self.check_subdomain(client, domain, sd) for sd in subdomains]
            results = await asyncio.gather(*tasks)
        return list(filter(None, results))

    @async_retry(max_retries=settings.MAX_RETRIES, delay=settings.RETRY_DELAY)
    async def fetch_content(self, url: str, max_redirects: int = 5, visited: set = None) -> str:
        """Получение контента страницы с обработкой редиректов"""
        if visited is None:
            visited = set()
        
        if url in visited:
            logger.warning(f"Cyclic redirect detected for URL: {url}")
            return ""
        
        if len(visited) >= max_redirects:
            logger.warning(f"Max redirects ({max_redirects}) reached for URL: {url}")
            return ""
        
        try:
            async with httpx.AsyncClient(follow_redirects=False) as client:  # Отключаем автоматические редиректы
                response = await client.get(
                    url,
                    timeout=settings.REQUEST_TIMEOUT,
                    headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
                )
                
                # Обработка редиректов вручную
                if response.status_code in (301, 302, 303, 307, 308):
                    redirect_url = response.headers.get("Location")
                    if not redirect_url:
                        logger.warning(f"No Location header in redirect response for {url}")
                        return ""
                    
                    # Преобразуем относительный URL в абсолютный
                    if not redirect_url.startswith(("http://", "https://")):
                        from urllib.parse import urljoin
                        redirect_url = urljoin(url, redirect_url)
                    
                    logger.info(f"Redirecting from {url} to {redirect_url}")
                    return await self.fetch_content(redirect_url, max_redirects, visited | {url})
                
                response.raise_for_status()
                return response.text
        except httpx.HTTPStatusError as e:
            logger.warning(f"HTTP error for {url}: {str(e)}")
            return await self._dynamic_fetch(url)
        except Exception as e:
            logger.error(f"Failed to fetch {url}: {str(e)}")
            return ""

    async def _dynamic_fetch(self, url: str) -> str:
        """Асинхронный запуск Selenium"""
        loop = asyncio.get_event_loop()
        try:
            return await loop.run_in_executor(None, self._sync_selenium_fetch, url)
        except Exception as e:
            logger.error(f"Selenium error: {str(e)}")
            return ""

    def _sync_selenium_fetch(self, url: str) -> str:
        """Синхронный код Selenium"""
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        driver = webdriver.Chrome(options=options)
        try:
            driver.get(url)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            return driver.page_source
        finally:
            driver.quit()

    @timing_decorator
    async def crawl_pages(self, base_url: str, domain: str) -> Set[str]:
        """Поиск всех страниц на сайте"""
        initial_content = await self.fetch_content(base_url)
        soup = BeautifulSoup(initial_content, "html.parser")
        urls = {base_url}

        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.startswith("/"):
                href = f"{base_url}{href}"
            if domain in href and href not in urls:
                urls.add(href)
        return urls

    async def save_content(self, pages: Set[str], output_path: Path) -> None:
        """Сохранение контента с проверкой валидности"""
        lines_count = 0
        with output_path.open("w", encoding="utf-8") as f:
            for page in pages:
                content = await self.fetch_content(page)
                if not content:
                    logger.warning(f"Skipping empty content for {page}")
                    continue
                
                try:
                    cleaned = clean_html_content(content)
                except Exception as e:
                    logger.error(f"Failed to clean HTML for {page}: {str(e)}")
                    continue
                
                if lines_count >= settings.MAX_CONTENT_LINES:
                    break
                
                f.write(f"=== {page} ===\n{cleaned}\n\n")
                lines_count += cleaned.count("\n") + 1

    @timing_decorator
    async def process_site(self, url: str) -> dict:
        """Основной метод обработки сайта"""
        domain = self.extract_domain(url)
        subdomains = await self.scan_subdomains(domain)
        pages = await self.crawl_pages(f"https://{domain}", domain)
        
        content_file = self.results_dir / f"{domain}.txt"
        await self.save_content(pages, content_file)
        
        return {
            "domain": domain,
            "subdomains": subdomains,
            "pages_found": len(pages),
            "content_file": str(content_file)
        }

app/utils/helpers.py

# app/utils/helpers.py
import logging
import re
import time
import asyncio
from pathlib import Path
from typing import List, Optional, Callable
from functools import wraps
import httpx

def setup_logger(name: str = "app") -> logging.Logger:
    """Настройка логгера для приложения"""
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)

    if not logger.handlers:
        logger.addHandler(console_handler)

    return logger

def normalize_url(url: str, base_domain: str) -> str:
    """Нормализация URL-адресов"""
    if url.startswith("/"):
        return f"https://{base_domain}{url}"
    if not url.startswith(("http://", "https://")):
        return f"https://{url}"
    return url

def is_valid_url(url: str) -> bool:
    """Проверка валидности URL"""
    url_pattern = re.compile(
        r"^(https?://)?"
        r"([a-zA-Z0-9-]+\.)*[a-zA-Z0-9-]+\.[a-zA-Z]{2,}"
        r"(:[0-9]{1,5})?"
        r"(/.*)?$"
    )
    return bool(url_pattern.match(url))

def clean_html_content(html: str) -> str:
    """Очистка HTML-контента"""
    from bs4 import BeautifulSoup
    try:
        soup = BeautifulSoup(html, "html.parser")
        
        for tag in ["script", "style", "nav", "footer", "header"]:
            for element in soup.find_all(tag):
                element.decompose()
        
        text = "\n".join(
            [p.get_text(strip=True) for p in soup.find_all(["p", "div", "section"])]
        )
        return "\n".join(line.strip() for line in text.splitlines() if line.strip())
    except Exception as e:
        return ""
        
def async_retry(max_retries: int = 3, delay: float = 1.0) -> Callable:
    """Декоратор для повторных асинхронных попыток"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return await func(*args, **kwargs)
                except (httpx.RequestError, Exception) as e:
                    retries += 1
                    if retries >= max_retries:
                        raise
                    await asyncio.sleep(delay)
        return wrapper
    return decorator

def validate_domain(domain: str) -> bool:
    """Проверка валидности домена"""
    domain_pattern = re.compile(
        r"^([a-zA-Z0-9-]+\.)*[a-zA-Z0-9-]+\.[a-zA-Z]{2,}$"
    )
    return bool(domain_pattern.match(domain))

def chunk_text(text: str, max_words: int = 300) -> List[str]:
    """Разделение текста на части по количеству слов"""
    words = text.split()
    return [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

def file_operations(file_path: Path) -> None:
    """Декоратор для обработки файловых операций"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                file_path.parent.mkdir(parents=True, exist_ok=True)
                return await func(*args, **kwargs)
            except (IOError, OSError) as e:
                logger = setup_logger()
                logger.error(f"File operation error: {str(e)}")
                raise
        return wrapper
    return decorator

def timing_decorator(func: Callable) -> Callable:
    """Декоратор для измерения времени выполнения"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        duration = time.time() - start_time
        logger = setup_logger()
        logger.info(f"{func.__name__} executed in {duration:.2f} seconds")
        return result

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start_time
        logger = setup_logger()
        logger.info(f"{func.__name__} executed in {duration:.2f} seconds")
        return result

    return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper

app/main.py

from fastapi import FastAPI
from app.controllers.parser import router as parse_router
from app.utils.helpers import setup_logger

logger = setup_logger(__name__)

app = FastAPI()
app.include_router(parse_router)

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting FastAPI application...")
    uvicorn.run(app, host="127.0.0.1", port=3000)