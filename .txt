app/main.py
from fastapi import FastAPI
import uvicorn

from config import main_config
from config.routes_config import api_routes
from app.hello.hello_routes import hello_router
from app.parse.parse_routes import parse_router


def main():
    try:
        app = FastAPI()

        api_prefix = api_routes.prefix if main_config.isDev else ''

        hello_glob = api_routes.hello.glob
        hello_prefix = f'{api_prefix}{hello_glob}'

        parser_glob = api_routes.parse.glob
        parser_prefix = f'{api_prefix}{parser_glob}'

        app.include_router( hello_router, prefix = hello_prefix )
        app.include_router( parse_router, prefix = parser_prefix )

        uvicorn.run(
            app    = app,
            host   = main_config.HOST,
            port   = main_config.PORT,
            reload = main_config.isDev,
        )
    except Exception as ex:
        print(ex)
    finally:
        print('Сервер закончил работу!')

if __name__ == "__main__":
    main()


app/config/settings.py
from dotenv import load_dotenv
load_dotenv()

from pathlib import Path
from pydantic import field_validator
from pydantic_settings import BaseSettings
from pydantic import ConfigDict

class Settings(BaseSettings):
    RESULTS_DIR: Path = Path("C:\\Users\\ta1\\Documents\\projects\\scaner\\app\\results")
    SUBDOMAINS_FILE: Path = Path("C:\\Users\\ta1\\Documents\\projects\\scaner\\app\\subdomains.txt")
    MAX_CONTENT_LINES: int = 500
    REQUEST_TIMEOUT: int = 15
    DEEPSEEK_API_KEY: str
    DEEPSEEK_MODEL: str = "google/gemma-3-12b-it:free"
    MAX_RETRIES: int = 3
    RETRY_DELAY: float = 1.0

    model_config = ConfigDict(
        env_file="app/.env",
        env_file_encoding="utf-8"
    )

    @field_validator("RESULTS_DIR", "SUBDOMAINS_FILE", mode="before")
    @classmethod
    def resolve_paths(cls, value: str | Path) -> Path:
        if isinstance(value, str):
            return Path(value).resolve()
        return value

settings = Settings()

app/controllers/parser.py
from fastapi import APIRouter, HTTPException
from pathlib import Path
from urllib.parse import unquote
from app.services.crawler import CrawlerService
from app.services.ai_processor import AIProcessor
from app.utils.helpers import setup_logger, is_valid_url
import traceback

logger = setup_logger(__name__)

router = APIRouter()

# Инициализация сервисов
crawler = CrawlerService()
ai = AIProcessor()

@router.get("/parse/{url:path}")
async def parse_website(url: str):
    try:
        decoded_url = unquote(url)
        logger.info(f"Processing: {decoded_url}")

        if not is_valid_url(decoded_url):
            raise HTTPException(status_code=400, detail="Некорректный URL")

        result = await crawler.process_site(decoded_url)
        content_file = Path(result["content_file"])

        if not content_file.exists():
            raise HTTPException(status_code=404, detail="Файл не найден")
            
        if content_file.stat().st_size < 100:
            raise HTTPException(status_code=422, detail="Недостаточно данных")

        description = await ai.generate_description(content_file)
        
        if "ошибка" in description.lower():
            raise HTTPException(status_code=500, detail=description)

        return {
            "domain": result["domain"],
            "subdomains_count": len(result["subdomains"]),
            "pages_found": result["pages_found"],
            "description": description,
            "content_file": str(content_file)
        }

    except HTTPException as he:
        logger.error(f"HTTP Error: {he.detail}")
        raise
    except Exception as e:
        logger.critical(f"Critical error: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail="Внутренняя ошибка сервера"
        )

app/services/ai_processor.py
import httpx
from pathlib import Path
from app.config.settings import settings
import asyncio
from app.utils.helpers import (
    setup_logger,
    async_retry,
    timing_decorator,
)

logger = setup_logger(__name__)

class AIProcessor:
    def __init__(self):
        self.api_key = settings.DEEPSEEK_API_KEY
        self.model = settings.DEEPSEEK_MODEL
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"

    @async_retry(max_retries=settings.MAX_RETRIES, delay=settings.RETRY_DELAY)
    @timing_decorator
    async def generate_description(self, content_file: Path) -> str:
        """Генерация описания сайта с обработкой ошибок API"""
        try:
            content = await self._read_limited_content_async(content_file)
            prompt = f"Опиши чем занимается компания на русском языке (должно быть не более 3 предложений): {content}"
            
            async with httpx.AsyncClient() as client:
                logger.debug(f"Using API Key: {self.api_key}")
                response = await client.post(
                    self.base_url,
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7,
                        "max_tokens": 500
                    },
                    timeout=30
                )
                
                response.raise_for_status()
                response_data = response.json()
                
                if "error" in response_data:
                    logger.error(f"API Error: {response_data['error']}")
                    return "Не удалось сгенерировать описание"
                    
                if "choices" not in response_data:
                    logger.error(f"Invalid API response: {response_data}")
                    return "Ошибка в структуре ответа API"
                    
                return response_data["choices"][0]["message"]["content"]
                
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error: {e.response.text}")
            return f"Ошибка API: {e.response.status_code}"
        except Exception as e:
            logger.error(f"Generation failed: {str(e)}")
            return "Ошибка генерации описания"

    async def _read_limited_content_async(self, file_path: Path, word_limit=300) -> str:
        """Асинхронное чтение файла с ограничением по количеству слов"""
        loop = asyncio.get_event_loop()
        
        content = await loop.run_in_executor(None, lambda: file_path.read_text(encoding="utf-8"))
        
        words = content.split()[word_limit:word_limit*2]
        return " ".join(words)

app/services/crawler.py
import asyncio
import httpx
from pathlib import Path
from typing import List, Set
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from app.config.settings import settings
from app.utils.helpers import (
    setup_logger,
    async_retry,
    timing_decorator,
    clean_html_content,
)

logger = setup_logger(__name__)

class CrawlerService:
    def __init__(self):
        self.results_dir = Path(settings.RESULTS_DIR)
        self.results_dir.mkdir(exist_ok=True)

    def extract_domain(self, url: str) -> str:
        """Извлечение домена из URL"""
        from tldextract import extract
        extracted = extract(url)
        return f"{extracted.domain}.{extracted.suffix}"

    @async_retry(max_retries=settings.MAX_RETRIES, delay=settings.RETRY_DELAY)
    async def check_subdomain(self, session: httpx.AsyncClient, domain: str, subdomain: str) -> str:
        """Проверка доступности поддомена"""
        url = f"https://{subdomain}.{domain}"
        try:
            response = await session.head(url, follow_redirects=True, timeout=settings.REQUEST_TIMEOUT)
            if response.status_code < 400:
                return str(response.url)
        except Exception as e:
            logger.warning(f"Failed to check subdomain {url}: {e}")
        return ""

    @timing_decorator
    async def scan_subdomains(self, domain: str) -> List[str]:
        """Сканирование поддоменов"""
        subdomains = Path(settings.SUBDOMAINS_FILE).read_text().splitlines()
        async with httpx.AsyncClient(verify=False) as client:
            tasks = [self.check_subdomain(client, domain, sd) for sd in subdomains]
            results = await asyncio.gather(*tasks)
        return list(filter(None, results))

    @async_retry(max_retries=settings.MAX_RETRIES, delay=settings.RETRY_DELAY)
    async def fetch_content(self, url: str, max_redirects: int = 5, visited: set = None) -> str:
        """Получение контента с улучшенной обработкой редиректов"""
        if visited is None:
            visited = set()
        
        if url in visited:
            logger.warning(f"Cyclic redirect detected: {url}")
            return ""
        
        if len(visited) >= max_redirects:
            logger.warning(f"Max redirects reached: {url}")
            return ""
        
        try:
            async with httpx.AsyncClient(follow_redirects=False) as client:
                response = await client.get(
                    url,
                    timeout=settings.REQUEST_TIMEOUT,
                    headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
                )
                
                if response.status_code in (301, 302, 303, 307, 308):
                    redirect_url = response.headers.get("Location")
                    if not redirect_url:
                        logger.warning(f"No Location header: {url}")
                        return ""
                    
                    if not redirect_url.startswith(("http://", "https://")):
                        from urllib.parse import urljoin
                        redirect_url = urljoin(url, redirect_url)
                    
                    logger.info(f"Redirect: {url} -> {redirect_url}")
                    return await self.fetch_content(redirect_url, max_redirects, visited | {url})
                
                response.raise_for_status()
                return response.text
                
        except httpx.HTTPStatusError as e:
            logger.warning(f"HTTP Error: {str(e)}")
            return await self._dynamic_fetch(url)
        except Exception as e:
            logger.error(f"Fetch failed: {str(e)}")
            return ""

    async def _dynamic_fetch(self, url: str) -> str:
        """Асинхронный запуск Selenium"""
        loop = asyncio.get_event_loop()
        try:
            return await loop.run_in_executor(None, self._sync_selenium_fetch, url)
        except Exception as e:
            logger.error(f"Selenium error: {str(e)}")
            return ""

    def _sync_selenium_fetch(self, url: str) -> str:
        """Синхронный код Selenium"""
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        driver = webdriver.Chrome(options=options)
        try:
            driver.get(url)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            return driver.page_source
        finally:
            driver.quit()

    @timing_decorator
    async def crawl_pages(self, base_url: str, domain: str) -> Set[str]:
        """Поиск всех страниц на сайте"""
        initial_content = await self.fetch_content(base_url)
        soup = BeautifulSoup(initial_content, "html.parser")
        urls = {base_url}

        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.startswith("/"):
                href = f"{base_url}{href}"
            if domain in href and href not in urls:
                urls.add(href)
        return urls

    async def save_content(self, pages: Set[str], output_path: Path) -> None:
        """Сохранение контента с проверкой валидности"""
        lines_count = 0
        with output_path.open("w", encoding="utf-8") as f:
            for page in pages:
                content = await self.fetch_content(page)
                if not content:
                    logger.warning(f"Skipping empty content for {page}")
                    continue
                
                try:
                    cleaned = clean_html_content(content)
                except Exception as e:
                    logger.error(f"Failed to clean HTML for {page}: {str(e)}")
                    continue
                
                if lines_count >= settings.MAX_CONTENT_LINES:
                    break
                
                f.write(f"=== {page} ===\n{cleaned}\n\n")
                lines_count += cleaned.count("\n") + 1

    @timing_decorator
    async def process_site(self, url: str) -> dict:
        """Основной метод обработки сайта"""
        domain = self.extract_domain(url)
        subdomains = await self.scan_subdomains(domain)
        pages = await self.crawl_pages(f"https://{domain}", domain)
        
        content_file = self.results_dir / f"{domain}.txt"
        await self.save_content(pages, content_file)
        
        return {
            "domain": domain,
            "subdomains": subdomains,
            "pages_found": len(pages),
            "content_file": str(content_file)
        }

app/utils/helpers.py
# app/utils/helpers.py
import logging
import re
import time
import asyncio
from pathlib import Path
from typing import List, Optional, Callable
from functools import wraps
import httpx

def setup_logger(name: str = "app") -> logging.Logger:
    """Настройка логгера для приложения"""
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)

    if not logger.handlers:
        logger.addHandler(console_handler)

    return logger

def normalize_url(url: str, base_domain: str) -> str:
    """Нормализация URL-адресов"""
    if url.startswith("/"):
        return f"https://{base_domain}{url}"
    if not url.startswith(("http://", "https://")):
        return f"https://{url}"
    return url

def is_valid_url(url: str) -> bool:
    """Проверка валидности URL"""
    url_pattern = re.compile(
        r"^(https?://)?"
        r"([a-zA-Z0-9-]+\.)*[a-zA-Z0-9-]+\.[a-zA-Z]{2,}"
        r"(:[0-9]{1,5})?"
        r"(/.*)?$"
    )
    return bool(url_pattern.match(url))

def clean_html_content(html: str) -> str:
    """Очистка HTML-контента"""
    from bs4 import BeautifulSoup
    try:
        soup = BeautifulSoup(html, "html.parser")
        
        for tag in ["script", "style", "nav", "footer", "header"]:
            for element in soup.find_all(tag):
                element.decompose()
        
        text = "\n".join(
            [p.get_text(strip=True) for p in soup.find_all(["p", "div", "section"])]
        )
        return "\n".join(line.strip() for line in text.splitlines() if line.strip())
    except Exception as e:
        return ""
        
def async_retry(max_retries: int = 3, delay: float = 1.0) -> Callable:
    """Декоратор для повторных асинхронных попыток"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return await func(*args, **kwargs)
                except (httpx.RequestError, Exception) as e:
                    retries += 1
                    if retries >= max_retries:
                        raise
                    await asyncio.sleep(delay)
        return wrapper
    return decorator

def validate_domain(domain: str) -> bool:
    """Проверка валидности домена"""
    domain_pattern = re.compile(
        r"^([a-zA-Z0-9-]+\.)*[a-zA-Z0-9-]+\.[a-zA-Z]{2,}$"
    )
    return bool(domain_pattern.match(domain))

def chunk_text(text: str, max_words: int = 300) -> List[str]:
    """Разделение текста на части по количеству слов"""
    words = text.split()
    return [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

def file_operations(file_path: Path) -> None:
    """Декоратор для обработки файловых операций"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                file_path.parent.mkdir(parents=True, exist_ok=True)
                return await func(*args, **kwargs)
            except (IOError, OSError) as e:
                logger = setup_logger()
                logger.error(f"File operation error: {str(e)}")
                raise
        return wrapper
    return decorator

def timing_decorator(func: Callable) -> Callable:
    """Декоратор для измерения времени выполнения"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        duration = time.time() - start_time
        logger = setup_logger()
        logger.info(f"{func.__name__} executed in {duration:.2f} seconds")
        return result

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start_time
        logger = setup_logger()
        logger.info(f"{func.__name__} executed in {duration:.2f} seconds")
        return result

    return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper

это мой код приложения на FastApi которое находит все доступные поддомены, парсит данные со страницы и с помощью api deepseek определяет тематику сайта
оно имеет у нас один эндпоинт GET запрос по которому возвращает такие данные 

{
    "domain": "pydantic.dev",
    "subdomains_count": 3,
    "pages_found": 20,
    "description": "\n\nPydantic – это популярная библиотека для проверки и обработки данных в Python. Она позволяет разработчикам легко и эффективно валидировать данные, используя простой и интуитивно понятный интерфейс. Благодаря удобному опыту разработки, Pydantic широко используется для создания надежных и качественных приложений.\n",
    "content_file": "/root/www/dns_scanner/results/pydantic.dev.txt"
}

Попрошу тебя:
1. добавить docker-compose 
2. записывать данные в базу - для этого еще добавить миграции
3.интегрировать с dishka

выведи мне полный код и структуру всего приложения, обязательно каждого файла целиком
